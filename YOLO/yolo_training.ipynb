{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Roboflow is used, copy the code produced by Roboflow to the cell below and run it to download the dataset.   \n",
    "Otherwise, if the dataset is already on the system there is no need to do this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"QwUVCugeIWxVA6THigIy\")\n",
    "project = rf.workspace(\"technion-institute-of-technology\").project(\"celegan-head\")\n",
    "version = project.version(5)\n",
    "dataset = version.download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load a base model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"yolov8s\" # can be one of the models stated at the ultraliytics documentation or a path to a trained model on the disk\n",
    "\n",
    "model = YOLO(model=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train The Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./CElegan-Head-5/data.yaml\"\n",
    "configuration_path = 'yolo_train_config.yaml'\n",
    "\n",
    "results = model.train(data=dataset_path, cfg=configuration_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export to faster format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"\" # Path of the final model\n",
    "\n",
    "model = YOLO(model=model_path)\n",
    "\n",
    "out = model.export(\n",
    "    format=\"engine\", # Use 'engine' for TensorRT (GPU), 'onnx' for ONNX (CPU/GPU).\n",
    "    imgsz=384,\n",
    "    half=False, # Enables FP16 (half-precision) quantization, reducing model size and potentially speeding up inference on supported hardware.\n",
    "    dynamic=False, # Allows dynamic input sizes for ONNX and TensorRT exports, enhancing flexibility in handling varying image dimensions\n",
    "    simplify=False, # Simplifies the model graph for ONNX exports, potentially improving performance and compatibility\n",
    "    verbose=False,\n",
    "    batch=8, # max supported batch size of the exported model\n",
    "    workspace=8, # max memory usage during the export process in GB\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
