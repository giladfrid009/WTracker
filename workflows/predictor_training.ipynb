{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Controller Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is used to create and train a neural network model for the purpose of predicting the worm movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix imports\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split, ChainDataset, ConcatDataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from wtracker.utils.path_utils import Files, join_paths\n",
    "from wtracker.utils.config_base import print_initialization\n",
    "from wtracker.neural.config import DatasetConfig, TrainConfig, IOConfig, LOSSES, OPTIMIZERS\n",
    "from wtracker.neural.dataset import NumpyDataset\n",
    "from wtracker.neural.mlp import MlpBlock, RMLP, WormPredictor\n",
    "from wtracker.neural.training import MLPTrainer\n",
    "from wtracker.neural.train_results import FitResult\n",
    "from wtracker.utils.gui_utils import UserPrompt\n",
    "\n",
    "\n",
    "pd.options.display.max_columns = 30\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the Model, Dataset and Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ User Input ################################\n",
    "\n",
    "# path to the log files, used to train the network, should be a list of paths \n",
    "# if None, a file dialog will open to select log files\n",
    "log_paths = ['D:/Guy_Gilad/Exp1_GuyGilad/logs_yolo/init_bboxes.csv'] \n",
    "\n",
    "# io config for 6 frame cycle (100ms) with 3 frames of pred and movement\n",
    "# io_config = IOConfig(\n",
    "#     input_frames=[0, -2, -9, -11, -18, -20, -27],  # list\n",
    "#     pred_frames=[9],  # list\n",
    "# )\n",
    "\n",
    "# io config for 15 frame cycle (200ms) with 3 frames of pred and movement\n",
    "io_config = IOConfig(\n",
    "    input_frames=[0, -3, -15, -18, -30, -33, -45],  # list\n",
    "    pred_frames=[3, 6, 9, 12],  # list\n",
    ")\n",
    "\n",
    "############################################################################\n",
    "\n",
    "if log_paths is None:\n",
    "    log_paths = []\n",
    "    while True:\n",
    "        path = UserPrompt.open_file(f\"Please select log file {len(log_paths)} to use for training, cancel if done\")\n",
    "        if len(path) == 0:\n",
    "            break\n",
    "        log_paths.append(path)\n",
    "\n",
    "dataset_config = DatasetConfig.from_io_config(io_config, log_paths) # create a dataset config object from the io_config and log_paths\n",
    "print(f\"dataset_config= {dataset_config.__dict__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a neural network model based on the dataset_config. Will be ignored if a model path is given by TrainConfig\n",
    "block_in_dim = 80\n",
    "block_dims = [40, 10, 40, 80]\n",
    "block_activations = [\"relu\"] * (len(block_dims))\n",
    "in_dim = 4 * len(dataset_config.input_frames)\n",
    "out_dim = 2 * len(dataset_config.pred_frames)\n",
    "\n",
    "model = RMLP(\n",
    "    in_dim=in_dim,\n",
    "    block_in_dim=block_in_dim,\n",
    "    block_dims=block_dims,\n",
    "    block_nonlins=block_activations,\n",
    "    n_blocks=4,\n",
    "    out_dim=out_dim,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we wrap the model in a WormPredictor object, which will hold the io_config for future use and distinguish it from general Neural Network models.\n",
    "model = WormPredictor(model, io_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model) # print the model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ User Input ################################\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    seed=42,  # int\n",
    "    dataset=dataset_config,  # Dataset\n",
    "    model=model,  # Union[nn.Module, str]\n",
    "    loss_fn=\"mse\",  # nn.Module\n",
    "    optimizer=\"adam\",  # Union[Optimizer, str]\n",
    "    device=f\"cpu\",  # str\n",
    "    log=True,  # bool\n",
    "    num_epochs=100,  # int\n",
    "    checkpoints=\"ResMLP(1)_config1.pt\",  # str\n",
    "    early_stopping=15,  # int\n",
    "    print_every=5,  # int\n",
    "    learning_rate=0.001,  # float\n",
    "    weight_decay=1e-05,  # float\n",
    "    batch_size=128,  # int\n",
    "    shuffle=True,  # bool\n",
    "    num_workers=0,  # int\n",
    "    train_test_split=0.8,  # float\n",
    ")\n",
    "\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run The Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for path in dataset_config.log_path:\n",
    "    # create a dataset according to the dataset_config\n",
    "    config = DatasetConfig.from_io_config(io_config, path)\n",
    "    datasets.append(NumpyDataset.create_from_config(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine datasets of all log files\n",
    "dataset = ConcatDataset(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "ds_train, ds_test = random_split(dataset, [train_config.train_test_split, 1 - train_config.train_test_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloaders\n",
    "dl_train = DataLoader(ds_train, batch_size=train_config.batch_size, shuffle=train_config.shuffle)\n",
    "dl_test = DataLoader(ds_test, batch_size=train_config.batch_size, shuffle=train_config.shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the loss object\n",
    "loss_fn = LOSSES[train_config.loss_fn]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the optimizer object\n",
    "lr = train_config.learning_rate\n",
    "weight_decay = train_config.weight_decay\n",
    "optimizer = OPTIMIZERS[train_config.optimizer](model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the trainer object\n",
    "device = torch.device(train_config.device)\n",
    "trainer = MLPTrainer(model, loss_fn, optimizer, device=device, log=train_config.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "epochs = train_config.num_epochs\n",
    "checkpoints = train_config.checkpoints\n",
    "early_stopping = train_config.early_stopping\n",
    "print_every = train_config.print_every\n",
    "\n",
    "trainer.fit(\n",
    "    dl_train=dl_train,\n",
    "    dl_test=dl_test,\n",
    "    num_epochs=epochs,\n",
    "    checkpoints=checkpoints,\n",
    "    print_every=print_every,\n",
    "    early_stopping=early_stopping,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the training configuration if logging is enabled\n",
    "if train_config.log:\n",
    "    train_config.save_pickle(join_paths(trainer.logger.log_dir, \"train_config.pkl\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
